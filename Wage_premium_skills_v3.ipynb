{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: linearmodels in /home/ubuntu/.local/lib/python3.10/site-packages (6.1)\n",
      "Requirement already satisfied: Cython>=3.0.10 in /home/ubuntu/.local/lib/python3.10/site-packages (from linearmodels) (3.1.1)\n",
      "Requirement already satisfied: formulaic>=1.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from linearmodels) (1.1.1)\n",
      "Requirement already satisfied: setuptools-scm[toml]<9.0.0,>=8.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from linearmodels) (8.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from linearmodels) (1.1.0)\n",
      "Requirement already satisfied: pyhdfe>=0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from linearmodels) (0.2.0)\n",
      "Requirement already satisfied: statsmodels>=0.13.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from linearmodels) (0.14.4)\n",
      "Requirement already satisfied: pandas>=1.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from linearmodels) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from linearmodels) (1.15.3)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from linearmodels) (2.2.6)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from formulaic>=1.0.0->linearmodels) (1.3.0)\n",
      "Requirement already satisfied: wrapt>=1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from formulaic>=1.0.0->linearmodels) (1.17.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from formulaic>=1.0.0->linearmodels) (4.13.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.4.0->linearmodels) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas>=1.4.0->linearmodels) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas>=1.4.0->linearmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: tomli>=1 in /home/ubuntu/.local/lib/python3.10/site-packages (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20 in /home/ubuntu/.local/lib/python3.10/site-packages (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (25.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from setuptools-scm[toml]<9.0.0,>=8.0.0->linearmodels) (59.6.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from statsmodels>=0.13.0->linearmodels) (1.0.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->linearmodels) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install linearmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyfixest in /home/ubuntu/.local/lib/python3.10/site-packages (0.29.0)\n",
      "Requirement already satisfied: tqdm>=4.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyfixest) (4.67.1)\n",
      "Requirement already satisfied: formulaic>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyfixest) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyfixest) (2.2.6)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyfixest) (0.9.0)\n",
      "Requirement already satisfied: great-tables>=0.10.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyfixest) (0.17.0)\n",
      "Requirement already satisfied: pandas>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyfixest) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.13.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyfixest) (0.13.2)\n",
      "Requirement already satisfied: numba>=0.58.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyfixest) (0.61.2)\n",
      "Requirement already satisfied: joblib<2,>=1.4.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyfixest) (1.5.1)\n",
      "Requirement already satisfied: narwhals>=1.13.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyfixest) (1.40.0)\n",
      "Requirement already satisfied: scipy>=1.6 in /home/ubuntu/.local/lib/python3.10/site-packages (from pyfixest) (1.15.3)\n",
      "Requirement already satisfied: interface-meta>=1.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from formulaic>=1.1.0->pyfixest) (1.3.0)\n",
      "Requirement already satisfied: wrapt>=1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from formulaic>=1.1.0->pyfixest) (1.17.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from formulaic>=1.1.0->pyfixest) (4.13.2)\n",
      "Requirement already satisfied: faicons>=0.2.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from great-tables>=0.10.0->pyfixest) (0.2.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/lib/python3/dist-packages (from great-tables>=0.10.0->pyfixest) (4.6.4)\n",
      "Requirement already satisfied: commonmark>=0.9.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from great-tables>=0.10.0->pyfixest) (0.9.1)\n",
      "Requirement already satisfied: htmltools>=0.4.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from great-tables>=0.10.0->pyfixest) (0.6.0)\n",
      "Requirement already satisfied: importlib-resources in /home/ubuntu/.local/lib/python3.10/site-packages (from great-tables>=0.10.0->pyfixest) (6.5.2)\n",
      "Requirement already satisfied: Babel>=2.13.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from great-tables>=0.10.0->pyfixest) (2.17.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/ubuntu/.local/lib/python3.10/site-packages (from numba>=0.58.0->pyfixest) (0.44.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas>=1.1.0->pyfixest) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas>=1.1.0->pyfixest) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.1.0->pyfixest) (2022.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from seaborn>=0.13.2->pyfixest) (3.10.3)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from htmltools>=0.4.1->great-tables>=0.10.0->pyfixest) (25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.13.2->pyfixest) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.13.2->pyfixest) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.13.2->pyfixest) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.13.2->pyfixest) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.13.2->pyfixest) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn>=0.13.2->pyfixest) (4.58.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.0->pyfixest) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyfixest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: adjustText in /home/ubuntu/.local/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/.local/lib/python3.10/site-packages (from adjustText) (1.15.3)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from adjustText) (2.2.6)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.10/site-packages (from adjustText) (3.10.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib->adjustText) (1.4.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib->adjustText) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib->adjustText) (1.3.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib->adjustText) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib->adjustText) (25.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib->adjustText) (4.58.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->adjustText) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib->adjustText) (0.12.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->adjustText) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in /home/ubuntu/.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (4.58.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.2->seaborn) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture:             x86_64\n",
      "  CPU op-mode(s):         32-bit, 64-bit\n",
      "  Address sizes:          48 bits physical, 48 bits virtual\n",
      "  Byte Order:             Little Endian\n",
      "CPU(s):                   8\n",
      "  On-line CPU(s) list:    0-7\n",
      "Vendor ID:                AuthenticAMD\n",
      "  Model name:             AMD EPYC 7571\n",
      "    CPU family:           23\n",
      "    Model:                1\n",
      "    Thread(s) per core:   2\n",
      "    Core(s) per socket:   4\n",
      "    Socket(s):            1\n",
      "    Stepping:             2\n",
      "    BogoMIPS:             4399.87\n",
      "    Flags:                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge m\n",
      "                          ca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall\n",
      "                           nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep\n",
      "                          _good nopl nonstop_tsc cpuid extd_apicid aperfmperf ts\n",
      "                          c_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_\n",
      "                          2 movbe popcnt aes xsave avx f16c rdrand hypervisor la\n",
      "                          hf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dno\n",
      "                          wprefetch topoext perfctr_core vmmcall fsgsbase bmi1 a\n",
      "                          vx2 smep bmi2 rdseed adx smap clflushopt sha_ni xsaveo\n",
      "                          pt xsavec xgetbv1 clzero xsaveerptr arat npt nrip_save\n",
      "Virtualization features:  \n",
      "  Hypervisor vendor:      KVM\n",
      "  Virtualization type:    full\n",
      "Caches (sum of all):      \n",
      "  L1d:                    128 KiB (4 instances)\n",
      "  L1i:                    256 KiB (4 instances)\n",
      "  L2:                     2 MiB (4 instances)\n",
      "  L3:                     8 MiB (1 instance)\n",
      "NUMA:                     \n",
      "  NUMA node(s):           1\n",
      "  NUMA node0 CPU(s):      0-7\n",
      "Vulnerabilities:          \n",
      "  Gather data sampling:   Not affected\n",
      "  Itlb multihit:          Not affected\n",
      "  L1tf:                   Not affected\n",
      "  Mds:                    Not affected\n",
      "  Meltdown:               Not affected\n",
      "  Mmio stale data:        Not affected\n",
      "  Reg file data sampling: Not affected\n",
      "  Retbleed:               Mitigation; untrained return thunk; SMT vulnerable\n",
      "  Spec rstack overflow:   Vulnerable: Safe RET, no microcode\n",
      "  Spec store bypass:      Vulnerable\n",
      "  Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointe\n",
      "                          r sanitization\n",
      "  Spectre v2:             Mitigation; Retpolines; STIBP disabled; RSB filling; P\n",
      "                          BRSB-eIBRS Not affected; BHI Not affected\n",
      "  Srbds:                  Not affected\n",
      "  Tsx async abort:        Not affected\n"
     ]
    }
   ],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import snowflake.connector as snow\n",
    "import statsmodels.api as sm\n",
    "import gc\n",
    "import warnings\n",
    "import os\n",
    "from linearmodels import PanelOLS, PooledOLS\n",
    "import pickle\n",
    "import config\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from adjustText import adjust_text\n",
    "from pyfixest import feols\n",
    "import seaborn as sns\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:snowflake.connector.connection:Snowflake Connector for Python Version: 3.15.0, Python Version: 3.10.12, Platform: Linux-6.8.0-1033-aws-x86_64-with-glibc2.35\n",
      "INFO:snowflake.connector.connection:Connecting to GLOBAL Snowflake domain\n"
     ]
    }
   ],
   "source": [
    "# Set up your connection parameters\n",
    "user = config.credentials['USERNAME']\n",
    "password = config.credentials['PASSWORD']\n",
    "account = 'PCA67849'\n",
    "warehouse = config.credentials['WAREHOUSE']\n",
    "\n",
    "# Connect to Snowflake\n",
    "conn = snow.connect(\n",
    "    user=user,\n",
    "    password=password,\n",
    "    account=account,\n",
    "    warehouse=warehouse,\n",
    "    database='',\n",
    "    schema=''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:snowflake.connector.vendored.urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /5uhk-s-p2sw7210/results/01be25e0-0809-116d-0057-1f8302103a82_0/main/data_0_1_7_1?x-amz-server-side-encryption-customer-algorithm=AES256&response-content-encoding=gzip&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250804T220725Z&X-Amz-SignedHeaders=host&X-Amz-Expires=21599&X-Amz-Credential=AKIAQS4LYUGWL7CPRRUA%2F20250804%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=****\n"
     ]
    }
   ],
   "source": [
    "# Start query with half a million sample\n",
    "combined_query = f\"\"\"\n",
    "WITH base_postings AS (\n",
    "    -- Get base postings data with sampling (500,000 per SOC per year)\n",
    "    SELECT \n",
    "        ID,\n",
    "        POSTED,\n",
    "        CASE \n",
    "            WHEN SALARY IS NOT NULL THEN SALARY\n",
    "            ELSE (SALARY_TO + SALARY_FROM) / 2 \n",
    "        END as salary_imputed,\n",
    "        ONET_2019,\n",
    "        TITLE_CLEAN,\n",
    "        SOC_2021_2,\n",
    "        SOC_2021_2_NAME,\n",
    "        SOC_2021_5,           -- Added SOC_2021_5 for fixed effects\n",
    "        SOC_2021_5_NAME,      -- Added SOC_2021_5_NAME for readability\n",
    "        COALESCE(MIN_EDULEVELS_NAME, 'Missing') as MIN_EDULEVELS_NAME,\n",
    "        COALESCE(MIN_YEARS_EXPERIENCE, 0) as MIN_YEARS_EXPERIENCE,\n",
    "        YEAR(POSTED) as YEAR\n",
    "    FROM BGI_POSTINGS_BACKUPS.FEB_25.US_POSTINGS\n",
    "    WHERE YEAR(POSTED) BETWEEN 2022 AND 2024\n",
    "        AND ONET_2019 IS NOT NULL\n",
    "        AND (SALARY_TO > 0 OR SALARY > 0 OR SALARY_FROM > 0)\n",
    "        AND ARRAY_CONTAINS('Company'::VARIANT, SOURCE_TYPES)\n",
    "        AND IS_INTERNSHIP = FALSE\n",
    "        AND COMPANY_IS_STAFFING = FALSE\n",
    "        AND SOC_2021_2 IN ('11-0000', '13-0000', '15-0000', '17-0000', '19-0000') -- white collar\n",
    "        -- AND SOC_2021_2 IN ('47-0000', '49-0000', '51-0000') -- blue collar\n",
    "        -- Sample 500,000 per SOC per year\n",
    "        -- QUALIFY ROW_NUMBER() OVER (PARTITION BY YEAR(POSTED), SOC_2021_2 ORDER BY RANDOM()) <= 500000\n",
    "),\n",
    "skill_counts AS (\n",
    "    -- Calculate skill counts (removed minimum threshold)\n",
    "    SELECT \n",
    "        SKILL_NAME,\n",
    "        COUNT(DISTINCT ps.ID) as skill_count\n",
    "    FROM BGI_POSTINGS_BACKUPS.FEB_25.US_POSTINGS_SKILLS ps\n",
    "    INNER JOIN base_postings bp ON ps.ID = bp.ID\n",
    "    GROUP BY SKILL_NAME\n",
    "    HAVING COUNT(DISTINCT ps.ID) >= 500\n",
    ")\n",
    "\n",
    "-- Final query\n",
    "SELECT \n",
    "    bp.*,\n",
    "    ps.SKILL_NAME,\n",
    "    sc.skill_count\n",
    "FROM base_postings bp\n",
    "LEFT JOIN BGI_POSTINGS_BACKUPS.FEB_25.US_POSTINGS_SKILLS ps ON bp.ID = ps.ID\n",
    "LEFT JOIN skill_counts sc ON ps.SKILL_NAME = sc.SKILL_NAME\n",
    "\"\"\"\n",
    "# Execute query and get data\n",
    "reg_df = pd.read_sql(combined_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the skills list\n",
    "skills_query = \"\"\"\n",
    "SELECT \"Skill\" \n",
    "FROM TEMPORARY_DATA.JNANIA.\"ADE_LC_skills_v5_2025_03_10\"\n",
    "\"\"\"\n",
    "skills = pd.read_sql(skills_query, conn)['Skill'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering valid skills...\n",
      "Number of skills to process: 101\n"
     ]
    }
   ],
   "source": [
    "# Calculate skill statistics\n",
    "skill_stats = reg_df[reg_df['SKILL_NAME'].isin(skills)].groupby('SKILL_NAME').agg({\n",
    "    'SOC_2021_2': 'nunique',  # Changed from ONET_2019 to SOC_2021_2\n",
    "    'MIN_EDULEVELS_NAME': 'nunique',\n",
    "    'ID': 'count'\n",
    "}).rename(columns={\n",
    "    'SOC_2021_2': 'soc_count',  # Renamed from occ_count to soc_count\n",
    "    'MIN_EDULEVELS_NAME': 'ed_count',\n",
    "    'ID': 'obs_count'\n",
    "})\n",
    "\n",
    "# Filter valid skills\n",
    "print(\"Filtering valid skills...\")\n",
    "include_skills = skill_stats[\n",
    "    (skill_stats['soc_count'] > 1) & \n",
    "    (skill_stats['ed_count'] > 1)  # Fixed: removed 'of'\n",
    "].index.tolist()\n",
    "print(f\"Number of skills to process: {len(include_skills)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to get share of jobs that have ade skills\n",
    "skill_occ_query = f\"\"\"\n",
    "WITH base_postings AS (\n",
    "    SELECT COUNT(DISTINCT ID) AS total_postings\n",
    "    FROM BGI_POSTINGS_BACKUPS.FEB_25.US_POSTINGS\n",
    "    WHERE YEAR(POSTED) BETWEEN 2022 AND 2024 --- pooling 3 years of data\n",
    "        AND SOC_2021_2 IS NOT NULL\n",
    "        AND SOC_2021_2 != '99-0000'\n",
    "        AND ARRAY_SIZE(SKILLS) > 0\n",
    "        AND (SALARY_TO > 0 OR SALARY > 0 OR SALARY_FROM > 0)\n",
    "        AND ARRAY_CONTAINS('Company'::VARIANT, SOURCE_TYPES)\n",
    "        AND IS_INTERNSHIP = FALSE\n",
    "        AND COMPANY_IS_STAFFING = FALSE\n",
    "),\n",
    "postings_with_ade_skills AS (\n",
    "    SELECT COUNT(DISTINCT bp.ID) AS skill_postings\n",
    "    FROM BGI_POSTINGS_BACKUPS.FEB_25.US_POSTINGS bp\n",
    "    JOIN BGI_POSTINGS_BACKUPS.FEB_25.US_POSTINGS_SKILLS ps ON bp.ID = ps.ID\n",
    "    WHERE YEAR(bp.POSTED) BETWEEN 2022 AND 2024\n",
    "        AND bp.SOC_2021_2 IS NOT NULL\n",
    "        AND bp.SOC_2021_2 != '99-0000'\n",
    "        AND ARRAY_SIZE(bp.SKILLS) > 0\n",
    "        AND (bp.SALARY_TO > 0 OR bp.SALARY > 0 OR bp.SALARY_FROM > 0)\n",
    "        AND ARRAY_CONTAINS('Company'::VARIANT, bp.SOURCE_TYPES)\n",
    "        AND bp.IS_INTERNSHIP = FALSE\n",
    "        AND bp.COMPANY_IS_STAFFING = FALSE\n",
    "        AND ps.SKILL_NAME IN ({', '.join([f\"'{skill}'\" for skill in skills])})\n",
    ")\n",
    "SELECT \n",
    "    bp.total_postings,\n",
    "    pws.skill_postings,\n",
    "    ROUND((pws.skill_postings * 100.0 / bp.total_postings), 2) AS ade_skill_share_pct\n",
    "FROM base_postings bp, postings_with_ade_skills pws;\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and get data\n",
    "skills_share = pd.read_sql(skill_occ_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOTAL_POSTINGS</th>\n",
       "      <th>SKILL_POSTINGS</th>\n",
       "      <th>ADE_SKILL_SHARE_PCT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6801838</td>\n",
       "      <td>2793188</td>\n",
       "      <td>41.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TOTAL_POSTINGS  SKILL_POSTINGS  ADE_SKILL_SHARE_PCT\n",
       "0         6801838         2793188                41.07"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get counts by SOC-5 level for each skill\n",
    "# soc5_counts = reg_df.groupby(['SKILL_NAME', 'SOC_2021_5', 'SOC_2021_5_NAME', 'SOC_2021_2', 'SOC_2021_2_NAME']).size().reset_index(name='count')\n",
    "\n",
    "# # Sort by count within each skill\n",
    "# soc5_counts_sorted = soc5_counts.sort_values(['SKILL_NAME', 'count'], ascending=[True, False])\n",
    "\n",
    "# # Get top 6 SOC-5 categories for each skill\n",
    "# top_soc5 = soc5_counts_sorted.groupby('SKILL_NAME').head(6)\n",
    "\n",
    "# # Add a percentage column to show distribution\n",
    "# top_soc5['percentage'] = top_soc5.groupby('SKILL_NAME')['count'].transform(lambda x: x / x.sum() * 100)\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Top 6 SOC-5 categories for each skill:\")\n",
    "# print(top_soc5[['SKILL_NAME', 'SOC_2021_5', 'SOC_2021_5_NAME', 'SOC_2021_2_NAME', 'count', 'percentage']])\n",
    "\n",
    "# # Also summarize by SOC-2 level\n",
    "# soc2_counts = reg_df.groupby(['SKILL_NAME', 'SOC_2021_2', 'SOC_2021_2_NAME']).size().reset_index(name='count')\n",
    "# soc2_counts['percentage'] = soc2_counts.groupby('SKILL_NAME')['count'].transform(lambda x: x / x.sum() * 100)\n",
    "# soc2_counts_sorted = soc2_counts.sort_values(['SKILL_NAME', 'count'], ascending=[True, False])\n",
    "\n",
    "# print(\"\\nDistribution by SOC-2 categories:\")\n",
    "# print(soc2_counts_sorted[['SKILL_NAME', 'SOC_2021_2', 'SOC_2021_2_NAME', 'count', 'percentage']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regression_dataset(skill, full_data): \n",
    "    \"\"\"\n",
    "    Creates regression dataset for a given skill with proper control group using SOC_2021_5 fixed effects\n",
    "    \n",
    "    Parameters:\n",
    "    skill (str): The skill to analyze\n",
    "    full_data (DataFrame): The complete dataset with job postings\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (regression_df, skill_df, nonskill_df) - The prepared datasets for regression\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Define needed columns to reduce memory usage\n",
    "        needed_columns = ['ID', 'SALARY_IMPUTED', 'YEAR', \n",
    "                          'SOC_2021_5', 'SOC_2021_5_NAME',\n",
    "                          'SOC_2021_2', 'SOC_2021_2_NAME', \n",
    "                          'MIN_EDULEVELS_NAME', 'MIN_YEARS_EXPERIENCE', 'SKILL_NAME']\n",
    "        \n",
    "        # Create subset of posts that have the skill (treatment group)\n",
    "        skill_filter = full_data['SKILL_NAME'] == skill\n",
    "        skill_df = full_data.loc[skill_filter, needed_columns].copy()\n",
    "        skill_posts_ids = skill_df['ID'].unique()  # Get unique IDs\n",
    "        \n",
    "        # Remove duplicates - this is important for accurate counting\n",
    "        skill_df = skill_df.drop_duplicates(\n",
    "            subset=['ID', 'SALARY_IMPUTED', 'YEAR',\n",
    "                    'SOC_2021_5', 'SOC_2021_2', \n",
    "                    'MIN_EDULEVELS_NAME', 'MIN_YEARS_EXPERIENCE']\n",
    "        )\n",
    "        skill_df['has_skill'] = 1\n",
    "        \n",
    "        # Treatment group count = len(skill_df)\n",
    "        \n",
    "        # Process control group in chunks\n",
    "        chunk_size = 10000  \n",
    "        nonskill_chunks = []\n",
    "        \n",
    "        for chunk_start in range(0, len(full_data), chunk_size):\n",
    "            chunk_end = min(chunk_start + chunk_size, len(full_data))\n",
    "            chunk = full_data.iloc[chunk_start:chunk_end]\n",
    "            \n",
    "            # Filter for control observations (must NOT have the current skill)\n",
    "            nonskill_chunk = chunk[~chunk['ID'].isin(skill_posts_ids)]\n",
    "            if len(nonskill_chunk) > 0:\n",
    "                nonskill_chunk = nonskill_chunk[needed_columns].copy()\n",
    "                # Remove duplicates for accurate counting\n",
    "                nonskill_chunk = nonskill_chunk.drop_duplicates(\n",
    "                    subset=['ID', 'SALARY_IMPUTED', 'YEAR',\n",
    "                            'SOC_2021_5', 'SOC_2021_2',\n",
    "                            'MIN_EDULEVELS_NAME', 'MIN_YEARS_EXPERIENCE']\n",
    "                )\n",
    "                nonskill_chunk['has_skill'] = 0\n",
    "                nonskill_chunks.append(nonskill_chunk)\n",
    "            \n",
    "            del chunk, nonskill_chunk\n",
    "            gc.collect()\n",
    "        \n",
    "        # Combine all control chunks\n",
    "        nonskill_df = pd.concat(nonskill_chunks) if nonskill_chunks else pd.DataFrame(columns=needed_columns)\n",
    "        \n",
    "        # Limit control size for efficiency\n",
    "        control_size = 3 * len(skill_df)\n",
    "        if len(nonskill_df) > control_size:\n",
    "            nonskill_df = nonskill_df.sample(control_size, random_state=42)\n",
    "        \n",
    "        # Control group count = len(nonskill_df)\n",
    "        \n",
    "        # Check if we have a valid control group\n",
    "        if len(nonskill_df) == 0:\n",
    "            print(f\"Warning: Empty control group for {skill}\")\n",
    "            return None, skill_df, nonskill_df\n",
    "            \n",
    "        # Combine datasets\n",
    "        reg_df = pd.concat([skill_df, nonskill_df])\n",
    "        \n",
    "        # Overall count = len(reg_df)\n",
    "        \n",
    "        # Convert variables as needed\n",
    "        reg_df['has_skill'] = reg_df['has_skill'].astype(float)\n",
    "        reg_df['log_salary'] = np.log(reg_df['SALARY_IMPUTED'].astype(float))\n",
    "    \n",
    "        # Additional verification to ensure counts are accurate\n",
    "        treatment_count = sum(reg_df['has_skill'] == 1)\n",
    "        control_count = sum(reg_df['has_skill'] == 0)\n",
    "        \n",
    "        # Verify treatment count matches original\n",
    "        if treatment_count != len(skill_df):\n",
    "            print(f\"Warning: Treatment count mismatch for {skill}. Expected {len(skill_df)}, got {treatment_count}\")\n",
    "            \n",
    "        # Verify control count matches limit (if applicable)\n",
    "        if len(nonskill_df) > control_size and control_count != control_size:\n",
    "            print(f\"Warning: Control count mismatch for {skill}. Expected {control_size}, got {control_count}\")\n",
    "    \n",
    "        return reg_df, skill_df, nonskill_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating regression dataset for skill {skill}:\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression_with_fixest(skill, reg_df, skill_df, nonskill_df, exp_as_discrete, soc_code, soc_title):\n",
    "    \"\"\"\n",
    "    Run regression for a specific SOC group using pyfixest with SOC_2021_5 fixed effects\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Store key metrics - these are correctly calculating the counts\n",
    "        n_obs = len(reg_df)  # Overall count\n",
    "        n_skill = len(skill_df) if skill_df is not None else 0  # Treatment count\n",
    "        n_control = len(nonskill_df) if nonskill_df is not None else 0  # Control count\n",
    "        \n",
    "        # Verification step - make sure counts are consistent\n",
    "        if reg_df is not None:\n",
    "            actual_treatment = sum(reg_df['has_skill'] == 1)\n",
    "            actual_control = sum(reg_df['has_skill'] == 0)\n",
    "            \n",
    "            if actual_treatment != n_skill:\n",
    "                print(f\"Warning: Treatment count mismatch in regression. Expected {n_skill}, got {actual_treatment}\")\n",
    "                n_skill = actual_treatment  # Update to actual count\n",
    "                \n",
    "            if actual_control != n_control:\n",
    "                print(f\"Warning: Control count mismatch in regression. Expected {n_control}, got {actual_control}\")\n",
    "                n_control = actual_control  # Update to actual count\n",
    "                \n",
    "            # Verify total matches\n",
    "            if actual_treatment + actual_control != n_obs:\n",
    "                print(f\"Warning: Total count mismatch. Treatment + Control = {actual_treatment + actual_control}, Total = {n_obs}\")\n",
    "        \n",
    "        # Check for collinearity within this specific SOC group\n",
    "        if soc_code != 'All':  # Only for specific SOC groups\n",
    "            # Check variation of has_skill within fixed effect categories\n",
    "            soc5_var = reg_df.groupby('SOC_2021_5')['has_skill'].nunique()\n",
    "            edu_var = reg_df.groupby('MIN_EDULEVELS_NAME')['has_skill'].nunique()\n",
    "            \n",
    "            # If has_skill doesn't vary within fixed effect categories, skip regression\n",
    "            if soc5_var.max() <= 1 or edu_var.max() <= 1:\n",
    "                print(f\"    Insufficient variation for {skill} in {soc_title}. Skipping regression.\")\n",
    "                return {\n",
    "                    'term': skill,\n",
    "                    'soc_code': soc_code,\n",
    "                    'soc_title': soc_title,\n",
    "                    'estimate': None,\n",
    "                    'std.error': None,\n",
    "                    'p.value': None,\n",
    "                    'sig': 0,\n",
    "                    'n_obs': n_obs,\n",
    "                    'n_skill': n_skill,\n",
    "                    'n_control': n_control,\n",
    "                    'exp_as_discrete': exp_as_discrete\n",
    "                }\n",
    "        \n",
    "        # Convert categorical variables to strings for pyfixest\n",
    "        reg_df['SOC_2021_5'] = reg_df['SOC_2021_5'].astype(str)\n",
    "        reg_df['YEAR'] = reg_df['YEAR'].astype(str)\n",
    "        reg_df['MIN_EDULEVELS_NAME'] = reg_df['MIN_EDULEVELS_NAME'].astype(str)\n",
    "        \n",
    "        # Prepare fixed effect formula based on whether experience is discrete\n",
    "        if exp_as_discrete:\n",
    "            reg_df['MIN_YEARS_EXPERIENCE'] = reg_df['MIN_YEARS_EXPERIENCE'].astype(str)\n",
    "            formula = \"log_salary ~ has_skill | SOC_2021_5 + YEAR + MIN_EDULEVELS_NAME + MIN_YEARS_EXPERIENCE\"\n",
    "        else:\n",
    "            reg_df['MIN_YEARS_EXPERIENCE'] = reg_df['MIN_YEARS_EXPERIENCE'].astype(float)\n",
    "            formula = \"log_salary ~ has_skill + MIN_YEARS_EXPERIENCE | SOC_2021_5 + YEAR + MIN_EDULEVELS_NAME\"\n",
    "        \n",
    "        # Try-except block for the regression to catch collinearity errors\n",
    "        try:\n",
    "            # Run fixed effects regression using pyfixest\n",
    "            results = feols(formula, data=reg_df, vcov=\"HC1\")\n",
    "            \n",
    "            # Extract results - Using methods\n",
    "            try:\n",
    "                # Get coefficients dictionary \n",
    "                coef_dict = results.coef()\n",
    "                estimate = coef_dict.get('has_skill', None)\n",
    "                \n",
    "                # Get standard errors\n",
    "                se_dict = results.se()\n",
    "                std_error = se_dict.get('has_skill', None)\n",
    "                \n",
    "                # Get p-values\n",
    "                pvalue_dict = results.pvalue()\n",
    "                p_value = pvalue_dict.get('has_skill', None)\n",
    "                \n",
    "                # Calculate dollar premiums\n",
    "                if estimate is not None:\n",
    "                    mean_salary = reg_df[reg_df['has_skill'] == 0]['SALARY_IMPUTED'].mean()\n",
    "                    percentage_premium = (np.exp(estimate) - 1) * 100\n",
    "                    dollar_premium = (np.exp(estimate) - 1) * mean_salary\n",
    "                else:\n",
    "                    mean_salary = None\n",
    "                    percentage_premium = None\n",
    "                    dollar_premium = None\n",
    "                    \n",
    "            except Exception as method_error:\n",
    "                print(f\"Method access failed: {method_error}\")\n",
    "                estimate, std_error, p_value = None, None, None\n",
    "                mean_salary, percentage_premium, dollar_premium = None, None, None\n",
    "                \n",
    "        except ValueError as ve:\n",
    "            if \"All variables are collinear\" in str(ve):\n",
    "                print(f\"    Collinearity detected for {skill} in {soc_title}. Skipping regression.\")\n",
    "                return {\n",
    "                    'term': skill,\n",
    "                    'soc_code': soc_code,\n",
    "                    'soc_title': soc_title,\n",
    "                    'estimate': None,\n",
    "                    'std.error': None,\n",
    "                    'p.value': None,\n",
    "                    'sig': 0,\n",
    "                    'n_obs': n_obs,\n",
    "                    'n_skill': n_skill,\n",
    "                    'n_control': n_control,\n",
    "                    'exp_as_discrete': exp_as_discrete\n",
    "                }\n",
    "            else:\n",
    "                # Re-raise other ValueErrors\n",
    "                raise\n",
    "        \n",
    "        # Create result dictionary\n",
    "        result_dict = {\n",
    "            'term': skill,\n",
    "            'soc_code': soc_code,\n",
    "            'soc_title': soc_title,\n",
    "            'estimate': estimate,\n",
    "            'std.error': std_error,\n",
    "            'p.value': p_value,\n",
    "            'sig': 1 if p_value is not None and p_value < 0.05 else 0,\n",
    "            'percentage_premium': percentage_premium,\n",
    "            'dollar_premium': dollar_premium,\n",
    "            'mean_salary': mean_salary,\n",
    "            'n_obs': n_obs,          # Overall count\n",
    "            'n_skill': n_skill,      # Treatment count\n",
    "            'n_control': n_control,  # Control count\n",
    "            'exp_as_discrete': exp_as_discrete\n",
    "        }\n",
    "        \n",
    "        # Clean up\n",
    "        del results\n",
    "        gc.collect()\n",
    "        \n",
    "        return result_dict\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in regression for {skill} and SOC {soc_code}:\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        \n",
    "        # Return a structured error result\n",
    "        return {\n",
    "            'term': skill,\n",
    "            'soc_code': soc_code,\n",
    "            'soc_title': soc_title,\n",
    "            'estimate': None,\n",
    "            'std.error': None,\n",
    "            'p.value': None,\n",
    "            'sig': 0,\n",
    "            'percentage_premium': None,\n",
    "            'dollar_premium': None,\n",
    "            'mean_salary': None,\n",
    "            'n_obs': n_obs if 'n_obs' in locals() else 0,\n",
    "            'n_skill': n_skill if 'n_skill' in locals() else 0,\n",
    "            'n_control': n_control if 'n_control' in locals() else 0,\n",
    "            'exp_as_discrete': exp_as_discrete\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wage_premium_finder_by_soc(skill, reg_df, skill_df, nonskill_df, exp_as_discrete=False):\n",
    "    \"\"\"\n",
    "    Calculates wage premium for a given skill across different SOC groups.\n",
    "    Using SOC_2021_5 for fixed effects in the regression.\n",
    "    \n",
    "    Parameters:\n",
    "    skill (str): The skill to analyze\n",
    "    reg_df (DataFrame): Combined regression dataset\n",
    "    skill_df (DataFrame): Dataset of posts with the skill\n",
    "    nonskill_df (DataFrame): Dataset of posts without the skill\n",
    "    exp_as_discrete (bool): If True, experience is treated as discrete\n",
    "    \n",
    "    Returns:\n",
    "    list: Results of the wage premium analysis for each SOC group\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if reg_df is None or len(nonskill_df) == 0:\n",
    "            result = {\n",
    "                'term': skill,\n",
    "                'soc_code': 'All',\n",
    "                'soc_title': 'All SOCs',\n",
    "                'estimate': None,\n",
    "                'std.error': None,\n",
    "                'p.value': None,\n",
    "                'sig': 0,\n",
    "                'n_obs': len(skill_df) if skill_df is not None else 0,\n",
    "                'n_skill': len(skill_df) if skill_df is not None else 0,\n",
    "                'n_control': 0,\n",
    "                'exp_as_discrete': exp_as_discrete\n",
    "            }\n",
    "            return [result]\n",
    "        \n",
    "        # List to store results for each SOC group\n",
    "        results = []\n",
    "        \n",
    "        # Verify counts in the combined dataset\n",
    "        total_obs = len(reg_df)\n",
    "        total_skill = sum(reg_df['has_skill'] == 1)\n",
    "        total_control = sum(reg_df['has_skill'] == 0)\n",
    "        \n",
    "        if total_skill + total_control != total_obs:\n",
    "            print(f\"Warning: Count discrepancy for {skill}. Total: {total_obs}, Skill: {total_skill}, Control: {total_control}\")\n",
    "        \n",
    "        # Create a result for all SOCs combined first\n",
    "        all_soc_result = run_regression_with_fixest(\n",
    "            skill, reg_df, skill_df, nonskill_df, exp_as_discrete, \n",
    "            soc_code='All', soc_title='All SOCs'\n",
    "        )\n",
    "        results.append(all_soc_result)\n",
    "        \n",
    "        # Now run separate regressions for each SOC group\n",
    "        soc_groups = reg_df['SOC_2021_2'].unique()\n",
    "        \n",
    "        for soc_code in soc_groups:\n",
    "            # Skip if not enough data\n",
    "            soc_filter = reg_df['SOC_2021_2'] == soc_code\n",
    "            soc_reg_df = reg_df[soc_filter].copy()\n",
    "            \n",
    "            if len(soc_reg_df) < 100:  # Skip if too few observations\n",
    "                continue\n",
    "            \n",
    "            # Create subset dataframes for this SOC\n",
    "            soc_skill_df = skill_df[skill_df['SOC_2021_2'] == soc_code].copy() if skill_df is not None else None\n",
    "            soc_nonskill_df = nonskill_df[nonskill_df['SOC_2021_2'] == soc_code].copy() if nonskill_df is not None else None\n",
    "            \n",
    "            # Skip if not enough skill observations\n",
    "            if soc_skill_df is None or len(soc_skill_df) < 30:\n",
    "                continue\n",
    "                \n",
    "            # Verify counts for this SOC group\n",
    "            soc_total = len(soc_reg_df)\n",
    "            soc_skill_count = sum(soc_reg_df['has_skill'] == 1)\n",
    "            soc_control_count = sum(soc_reg_df['has_skill'] == 0)\n",
    "            \n",
    "            if soc_skill_count != len(soc_skill_df):\n",
    "                print(f\"Warning: Treatment count mismatch for {skill} in SOC {soc_code}\")\n",
    "            \n",
    "            if soc_control_count != len(soc_nonskill_df):\n",
    "                print(f\"Warning: Control count mismatch for {skill} in SOC {soc_code}\")\n",
    "            \n",
    "            # Get SOC title\n",
    "            soc_title = soc_reg_df['SOC_2021_2_NAME'].iloc[0] if len(soc_reg_df) > 0 else f\"SOC {soc_code}\"\n",
    "            \n",
    "            # Run regression for this SOC group\n",
    "            soc_result = run_regression_with_fixest(\n",
    "                skill, soc_reg_df, soc_skill_df, soc_nonskill_df, \n",
    "                exp_as_discrete, soc_code, soc_title\n",
    "            )\n",
    "            results.append(soc_result)\n",
    "            \n",
    "            # Free memory\n",
    "            del soc_reg_df, soc_skill_df, soc_nonskill_df\n",
    "            gc.collect()\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in wage premium analysis for skill {skill}:\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_by_soc(reg_df, skills, batch_size=5):\n",
    "    \"\"\"\n",
    "    Analyze wage premiums across different SOC groups using SOC_2021_5 fixed effects\n",
    "    \n",
    "    Parameters:\n",
    "    reg_df (DataFrame): The dataset with job postings\n",
    "    skills (list): List of skills to analyze\n",
    "    batch_size (int): Number of skills to process per batch\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Results of the analysis\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    # Process skills in batches\n",
    "    for i in range(0, len(skills), batch_size):\n",
    "        batch_skills = skills[i : i + batch_size]\n",
    "        batch_num = i//batch_size + 1\n",
    "        print(f\"\\nProcessing batch {batch_num}, skills {i+1} to {min(i+batch_size, len(skills))}\")\n",
    "        \n",
    "        for skill in batch_skills:\n",
    "            print(f\"  Processing skill: {skill}\")\n",
    "            \n",
    "            # Create regression datasets with SOC_2021_5 columns\n",
    "            reg_dataset, skill_df, nonskill_df = create_regression_dataset(skill, reg_df)\n",
    "            \n",
    "            if reg_dataset is not None:\n",
    "                # Track dataset sizes for verification\n",
    "                n_total = len(reg_dataset)\n",
    "                n_treatment = sum(reg_dataset['has_skill'] == 1)\n",
    "                n_control = sum(reg_dataset['has_skill'] == 0)\n",
    "                print(f\"    Dataset created - Total: {n_total}, Treatment: {n_treatment}, Control: {n_control}\")\n",
    "                \n",
    "                # Run regression with discrete experience across SOC groups\n",
    "                results = wage_premium_finder_by_soc(\n",
    "                    skill, reg_dataset, skill_df, nonskill_df, exp_as_discrete=True\n",
    "                )\n",
    "                if results is not None:\n",
    "                    all_results.extend(results)\n",
    "                    # Report results for all SOCs\n",
    "                    all_soc_result = next((r for r in results if r['soc_code'] == 'All'), None)\n",
    "                    if all_soc_result and all_soc_result['estimate'] is not None:\n",
    "                        print(f\"    Overall result: estimate={all_soc_result['estimate']:.4f}, p={all_soc_result['p.value']:.4f}\")\n",
    "                        print(f\"    Counts: n_obs={all_soc_result['n_obs']}, n_skill={all_soc_result['n_skill']}, n_control={all_soc_result['n_control']}\")\n",
    "                    else:\n",
    "                        print(f\"    No valid overall result.\")\n",
    "            else:\n",
    "                print(f\"    Could not create dataset for {skill}\")\n",
    "            \n",
    "            # Clear memory\n",
    "            del reg_dataset, skill_df, nonskill_df\n",
    "            gc.collect()\n",
    "        \n",
    "        # Force memory cleanup after batch\n",
    "        gc.collect()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    print(f\"Processing final results for SOC-specific analysis...\")\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    if len(results_df) == 0:\n",
    "        print(\"No results found!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Calculate additional metrics - handle None values\n",
    "    results_df['wage_diff_pct'] = results_df.apply(\n",
    "        lambda row: (np.exp(row['estimate']) - 1) if row['sig'] == 1 and row['estimate'] is not None else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Rank within each SOC group\n",
    "    results_df['rank'] = results_df.groupby('soc_code')['estimate'].rank(method='dense', ascending=False)\n",
    "    \n",
    "    # Add skill statistics\n",
    "    try:\n",
    "        # Try to handle different index formats\n",
    "        if isinstance(skill_stats.index, pd.Index):\n",
    "            stats_df = skill_stats.reset_index()\n",
    "            if 'SKILL_NAME' in stats_df.columns:\n",
    "                stats_df = stats_df.rename(columns={'SKILL_NAME': 'term'})\n",
    "            else:\n",
    "                stats_df = stats_df.rename(columns={'index': 'term'})\n",
    "        else:\n",
    "            stats_df = skill_stats.reset_index().rename(columns={'index': 'term'})\n",
    "            \n",
    "        # Only merge relevant stats to save memory\n",
    "        relevant_stats = stats_df[stats_df['term'].isin(results_df['term'].unique())]\n",
    "        results_df = results_df.merge(relevant_stats, on='term')\n",
    "        \n",
    "        # Free memory\n",
    "        del stats_df, relevant_stats\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Error adding skill statistics: {e}\")\n",
    "    \n",
    "    # Add skill categories\n",
    "    try:\n",
    "        # Load categories \n",
    "        skill_categories = pd.read_sql(\n",
    "            'SELECT * FROM TEMPORARY_DATA.JNANIA.\"ADE_LC_skills_v5_2025_03_10\"',\n",
    "            conn\n",
    "        )\n",
    "        \n",
    "        # Only get needed categories\n",
    "        categories_subset = skill_categories[skill_categories['Skill'].isin(results_df['term'].unique())]\n",
    "        results_df = results_df.merge(\n",
    "            categories_subset.rename(columns={'Skill': 'term'}),\n",
    "            on='term'\n",
    "        )\n",
    "        \n",
    "        # Calculate weighted wage premiums within SOC groups and categories\n",
    "        results_df['sh_skill_by_soc_category'] = results_df.groupby(['soc_code', 'Category Name'])['obs_count'].transform(\n",
    "            lambda x: x / x.sum()\n",
    "        )\n",
    "        results_df['wage_diff_pct_weighted'] = (\n",
    "            results_df['wage_diff_pct'] * results_df['sh_skill_by_soc_category']\n",
    "        )\n",
    "        \n",
    "        # Free memory\n",
    "        del categories_subset, skill_categories\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not add skill categories: {e}\")\n",
    "    \n",
    "    # # Add flag for small sample sizes\n",
    "    # results_df['small_sample'] = results_df['n_skill'] < 500\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_results(results_df):\n",
    "    \"\"\"\n",
    "    Process results from SOC_2021_5 fixed effects analysis and save to file\n",
    "    \n",
    "    Parameters:\n",
    "    results_df (DataFrame): The results dataframe to process and save\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (detailed_df, summary_df, filename) - The processed dataframes and output filename\n",
    "    \"\"\"\n",
    "    # Update filename to reflect the SOC_2021_5 fixed effects\n",
    "    filename = 'ADE_wage_premiums_by_soc_SOC5FE_fixest_blue_collar.csv'\n",
    "    \n",
    "    # Add timestamp to filename\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    filename = f'ADE_wage_premiums_by_soc_SOC5FE_fixest_{timestamp}.csv'\n",
    "    \n",
    "    # Save full results\n",
    "    results_df.to_csv(filename, index=False)\n",
    "    print(f\"SOC-specific results saved to {filename}\")\n",
    "    \n",
    "    # Create summary by SOC and category\n",
    "    try:\n",
    "        # Summary with all samples\n",
    "        summary_df = results_df.groupby(['soc_code', 'soc_title', 'Category Name']).agg({\n",
    "            'wage_diff_pct_weighted': 'sum',\n",
    "            'term': 'count',\n",
    "            'n_obs': 'sum',\n",
    "            'n_skill': 'sum',\n",
    "            'n_control': 'sum'\n",
    "        }).rename(columns={\n",
    "            'wage_diff_pct_weighted': 'category_wage_premium',\n",
    "            'term': 'skill_count',\n",
    "            'n_obs': 'total_postings',\n",
    "            'n_skill': 'treatment_postings',\n",
    "            'n_control': 'control_postings'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Add percent of postings that are in treatment group\n",
    "        summary_df['treatment_ratio'] = summary_df['treatment_postings'] / summary_df['total_postings']\n",
    "        \n",
    "        # Summary filename\n",
    "        summary_filename = f'ADE_wage_premiums_summary_by_soc_category_SOC5FE_{timestamp}.csv'\n",
    "        summary_df.to_csv(summary_filename, index=False)\n",
    "        print(f\"Summary by SOC and category saved to {summary_filename}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating summary: {e}\")\n",
    "        import traceback\n",
    "        print(traceback.format_exc())\n",
    "        summary_df = pd.DataFrame()\n",
    "    \n",
    "    return results_df, summary_df, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_skills_for_collinearity(reg_df, skills, min_occ_count=1, min_ed_count=1):\n",
    "    \"\"\"\n",
    "    Filter skills to remove those that would be collinear with SOC_2021_5 fixed effects\n",
    "    \"\"\"\n",
    "    # Filter the dataset to only include specified skills\n",
    "    skill_df = reg_df[reg_df['SKILL_NAME'].isin(skills)]\n",
    "    \n",
    "    # Group by skill and calculate counts\n",
    "    skill_stats = skill_df.groupby('SKILL_NAME').agg({\n",
    "        'SOC_2021_5': 'nunique',  # Using SOC_2021_5 instead of SOC_2021_2\n",
    "        'MIN_EDULEVELS_NAME': 'nunique',\n",
    "        'ID': 'count'\n",
    "    }).rename(columns={\n",
    "        'SOC_2021_5': 'soc5_count',  # Renamed to reflect SOC_2021_5\n",
    "        'MIN_EDULEVELS_NAME': 'ed_count',\n",
    "        'ID': 'obs_count'\n",
    "    })\n",
    "    \n",
    "    # Filter skills based on occupation and education counts\n",
    "    valid_skills = skill_stats[\n",
    "        (skill_stats['soc5_count'] > min_occ_count) & \n",
    "        (skill_stats['ed_count'] > min_ed_count)\n",
    "    ].index.tolist()\n",
    "    \n",
    "    # Add stats for skill counts by SOC_2021_2 for later use\n",
    "    soc2_counts = skill_df.groupby('SKILL_NAME')['SOC_2021_2'].nunique()\n",
    "    skill_stats['soc2_count'] = soc2_counts\n",
    "    \n",
    "    # Make this available globally\n",
    "    globals()['skill_stats'] = skill_stats\n",
    "    \n",
    "    print(f\"Filtered skills: {len(valid_skills)}/{len(skills)} remain after filtering\")\n",
    "    print(f\"  - Skills must appear in more than {min_occ_count} SOC_2021_5 categories\")\n",
    "    print(f\"  - Skills must appear in more than {min_ed_count} education levels\")\n",
    "    \n",
    "    return valid_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_soc_analysis_fixest(reg_df, skills, skill_stats, conn=None):\n",
    "    \"\"\"\n",
    "    Run analysis comparing wage premiums across SOC groups using pyfixest with SOC_2021_5 fixed effects.\n",
    "    \n",
    "    Parameters:\n",
    "    reg_df (DataFrame): The dataset with job postings\n",
    "    skills (list): List of skills to analyze\n",
    "    skill_stats (DataFrame): Skill statistics\n",
    "    conn: Database connection\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (detailed_df, summary_df, filename) - The two dataframes and filename\n",
    "    \"\"\"\n",
    "    # First, create a slimmer version of reg_df with only needed columns\n",
    "    needed_columns = ['ID', 'SALARY_IMPUTED', 'YEAR', \n",
    "                      'SOC_2021_5', 'SOC_2021_5_NAME',  # Added SOC_2021_5 fields\n",
    "                      'SOC_2021_2', 'SOC_2021_2_NAME',\n",
    "                      'MIN_EDULEVELS_NAME', 'MIN_YEARS_EXPERIENCE', 'SKILL_NAME']\n",
    "    reg_df_slim = reg_df[needed_columns].copy()\n",
    "    \n",
    "    # Filter skills to avoid collinearity issues - JUST LIKE R CODE\n",
    "    filtered_skills = filter_skills_for_collinearity(reg_df_slim, skills, min_occ_count=1, min_ed_count=1)\n",
    "    \n",
    "    # Make skill_stats available globally\n",
    "    globals()['skill_stats'] = skill_stats\n",
    "    \n",
    "    # Run SOC-specific analysis\n",
    "    print(\"\\n=== RUNNING SOC-SPECIFIC ANALYSIS WITH SOC_2021_5 FIXED EFFECTS USING PYFIXEST ===\")\n",
    "    results_df = analyze_by_soc(reg_df_slim, filtered_skills, batch_size=5)\n",
    "    detailed_df, summary_df, filename = process_and_save_results(results_df)\n",
    "    print(f\"Analysis complete. Results saved to {filename}\")\n",
    "    \n",
    "    # Generate summary statistics for each SOC group\n",
    "    soc_summary = detailed_df[detailed_df['soc_code'] != 'All'].groupby('soc_code').agg({\n",
    "        'soc_title': 'first',\n",
    "        'term': 'nunique',\n",
    "        'n_obs': 'sum',\n",
    "        'n_skill': 'sum',  # Added treatment count\n",
    "        'n_control': 'sum',  # Added control count\n",
    "        'wage_diff_pct': ['mean', 'median', 'min', 'max']\n",
    "    })\n",
    "    \n",
    "    print(\"\\nSummary by SOC group:\")\n",
    "    print(soc_summary)\n",
    "    \n",
    "    # Also generate summary by skill category\n",
    "    try:\n",
    "        category_summary = detailed_df[detailed_df['soc_code'] == 'All'].groupby('Category Name').agg({\n",
    "            'term': 'count',\n",
    "            'wage_diff_pct': ['mean', 'median', 'min', 'max']\n",
    "        }).sort_values(('wage_diff_pct', 'median'), ascending=False)\n",
    "        \n",
    "        print(\"\\nSummary by skill category (all SOCs):\")\n",
    "        print(category_summary)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate category summary: {e}\")\n",
    "    \n",
    "    return detailed_df, summary_df, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salary columns found: ['SALARY_IMPUTED']\n"
     ]
    }
   ],
   "source": [
    "# Check for salary-related columns\n",
    "salary_cols = [col for col in reg_df.columns if 'salary' in col.lower()]\n",
    "print(f\"Salary columns found: {salary_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2039198965.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Dont run regression\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Dont run regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution example\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting wage premium analysis by SOC code using SOC_2021_5 fixed effects with pyfixest...\")\n",
    "    \n",
    "    # Run the analysis with SOC_2021_5 fixed effects\n",
    "    detailed_df, summary_df, filename = run_soc_analysis_fixest(reg_df, include_skills, skill_stats, conn)\n",
    "    \n",
    "    print(f\"\\nAnalysis complete. Results saved to {filename}\")\n",
    "    print(f\"The two dataframes are available as 'detailed_df' and 'summary_df'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to run additional analysis without having to re-run the regressions\n",
    "detailed_df = pd.read_csv('ADE_wage_premiums_by_soc_SOC5FE_fixest_20250803_2219.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_df['wage_diff_pct_floor'] = np.maximum(detailed_df['wage_diff_pct'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soc5_by_soc2 = reg_df.groupby('SOC_2021_2')['SOC_2021_5'].nunique()\n",
    "print(\"SOC_2021_5 categories per SOC_2021_2 group:\")\n",
    "print(soc5_by_soc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incude counts SOC_2021_5 categories \n",
    "regression_soc5_counts = []\n",
    "\n",
    "# For the full dataset\n",
    "all_soc5_count = reg_df['SOC_2021_5'].nunique()\n",
    "regression_soc5_counts.append({\n",
    "    'soc_code': 'All', \n",
    "    'soc_title': 'All SOCs',\n",
    "    'soc5_count_used': all_soc5_count\n",
    "})\n",
    "\n",
    "# For each SOC_2021_2 group\n",
    "for soc_code in reg_df['SOC_2021_2'].unique():\n",
    "    soc_df = reg_df[reg_df['SOC_2021_2'] == soc_code]\n",
    "    soc_title = soc_df['SOC_2021_2_NAME'].iloc[0]\n",
    "    soc5_count = soc_df['SOC_2021_5'].nunique()\n",
    "    \n",
    "    regression_soc5_counts.append({\n",
    "        'soc_code': soc_code,\n",
    "        'soc_title': soc_title,\n",
    "        'soc5_count_used': soc5_count\n",
    "    })\n",
    "\n",
    "# Convert to dataframe and merge\n",
    "soc5_counts_df = pd.DataFrame(regression_soc5_counts)\n",
    "detailed_df = detailed_df.merge(soc5_counts_df, on=['soc_code', 'soc_title'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timestamp for the filename\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# Export to CSV with a descriptive filename\n",
    "output_filename = f'ADE_wage_premiums_by_soc_SOC5FE_with_soc5_counts_{timestamp}.csv'\n",
    "detailed_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Results with SOC_2021_5 counts exported to {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 skills with highest wage premium across all SOCs\n",
    "top_skills_overall = detailed_df[detailed_df['soc_code'] == 'All'].nlargest(10, 'wage_diff_pct')\n",
    "print(top_skills_overall[['term', 'wage_diff_pct']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's fix the data types\n",
    "data_to_clean = {\n",
    "    'term': 'string',\n",
    "    'soc_code': 'string',\n",
    "    'soc_title': 'string',\n",
    "    'estimate': 'float',\n",
    "    'std.error': 'float',\n",
    "    'p.value': 'float',\n",
    "    'sig': 'int',\n",
    "    'n_obs': 'int',\n",
    "    'n_skill': 'int',\n",
    "    'n_control': 'int',\n",
    "    'exp_as_discrete': 'bool',\n",
    "    'wage_diff_pct': 'float',\n",
    "    'wage_diff_pct_floor': 'float',\n",
    "    'rank': 'float',  # Keep as float since it might contain decimal rankings\n",
    "    'soc5_count': 'int',  # Updated from soc_count to soc5_count\n",
    "    'soc2_count': 'int',  # Added for SOC_2021_2 counts\n",
    "    'ed_count': 'int',\n",
    "    'obs_count': 'int',\n",
    "    'dollar_premium': 'float'  # Added for dollar premium\n",
    "    # 'small_sample': 'bool'  # Added for our small sample flag\n",
    "}\n",
    "\n",
    "# Convert the columns to appropriate types\n",
    "for col, dtype in data_to_clean.items():\n",
    "    if col in detailed_df.columns:\n",
    "        if dtype == 'float':\n",
    "            detailed_df[col] = pd.to_numeric(detailed_df[col], errors='coerce')\n",
    "        elif dtype == 'int':\n",
    "            detailed_df[col] = pd.to_numeric(detailed_df[col], errors='coerce').astype('Int64')\n",
    "        elif dtype == 'bool':\n",
    "            if detailed_df[col].dtype == 'object':\n",
    "                detailed_df[col] = detailed_df[col].map({'True': True, 'False': False})\n",
    "\n",
    "# Check if we already have category data\n",
    "if 'Category Name' not in detailed_df.columns:\n",
    "    # Query skill categories\n",
    "    category_query = \"\"\"\n",
    "    SELECT \"Skill\", \"Category Name\" \n",
    "    FROM TEMPORARY_DATA.JNANIA.\"ADE_LC_skills_v5_2025_03_10\"\n",
    "    \"\"\"\n",
    "    skill_categories = pd.read_sql(category_query, conn)\n",
    "    \n",
    "    # Merge with your detailed dataframe\n",
    "    df_with_categories = detailed_df.merge(\n",
    "        skill_categories,\n",
    "        left_on='term',\n",
    "        right_on='Skill',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Keep both term and Skill columns\n",
    "    df_with_categories = df_with_categories.drop('Skill', axis=1)\n",
    "    \n",
    "    # Check for any skills that didn't match\n",
    "    missing_categories = df_with_categories[df_with_categories['Category Name'].isna()]['term'].unique()\n",
    "    if len(missing_categories) > 0:\n",
    "        print(f\"Warning: {len(missing_categories)} skills did not match with categories:\")\n",
    "        print(missing_categories)\n",
    "else:\n",
    "    # Categories already added\n",
    "    df_with_categories = detailed_df\n",
    "\n",
    "# Generate timestamp for filenames\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# Save the results with categories\n",
    "output_file = f'ADE_wage_premiums_by_soc_SOC5FE_with_categories_{timestamp}.csv'\n",
    "df_with_categories.to_csv(output_file, index=False)\n",
    "print(f\"Saved detailed results with categories to {output_file}\")\n",
    "\n",
    "# Create a summary by SOC and category\n",
    "summary_by_category = df_with_categories.groupby(['soc_code', 'soc_title', 'Category Name']).agg({\n",
    "    'wage_diff_pct': ['mean', 'median'],\n",
    "    'dollar_premium': ['mean', 'median'],  # Added dollar premium aggregation\n",
    "    'term': 'nunique',\n",
    "    'n_obs': 'sum',\n",
    "    'n_skill': 'sum',\n",
    "    'n_control': 'sum'\n",
    "    # 'small_sample': 'mean'  # Percentage of skills with small samples\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the MultiIndex columns\n",
    "summary_by_category.columns = ['_'.join(col).strip('_') for col in summary_by_category.columns.values]\n",
    "\n",
    "# Rename columns for clarity\n",
    "summary_by_category = summary_by_category.rename(columns={\n",
    "    'wage_diff_pct_mean': 'avg_wage_premium_pct',\n",
    "    'wage_diff_pct_median': 'median_wage_premium_pct',\n",
    "    'dollar_premium_mean': 'avg_dollar_premium',\n",
    "    'dollar_premium_median': 'median_dollar_premium',\n",
    "    'term_nunique': 'skill_count',\n",
    "    'n_obs_sum': 'total_observations',\n",
    "    'n_skill_sum': 'treatment_observations',\n",
    "    'n_control_sum': 'control_observations',\n",
    "    # 'small_sample_mean': 'pct_small_samples'\n",
    "})\n",
    "\n",
    "# Add treatment ratio\n",
    "summary_by_category['treatment_ratio'] = summary_by_category['treatment_observations'] / summary_by_category['total_observations']\n",
    "\n",
    "# Save the category summary\n",
    "summary_file = f'ADE_wage_premiums_summary_by_category_SOC5FE_{timestamp}.csv'\n",
    "summary_by_category.to_csv(summary_file, index=False)\n",
    "print(f\"Saved category summary to {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_by_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for statistically significant results (p-value < 0.05)\n",
    "significant_results = df_with_categories[df_with_categories['p.value'] < 0.05]\n",
    "# # Also filter out small samples if desired\n",
    "# significant_reliable_results = significant_results[~significant_results['small_sample']]\n",
    "\n",
    "# Create a timestamp for filenames\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# Create a summary by SOC and category with only significant results\n",
    "summary_by_category_sig = significant_results.groupby(['soc_code', 'soc_title', 'Category Name']).agg({\n",
    "    'wage_diff_pct': ['mean', 'median', 'min', 'max'],\n",
    "    'dollar_premium': ['mean', 'median', 'min', 'max'],  # Added dollar premium\n",
    "    'mean_salary': ['mean', 'median', 'min', 'max'],     # Added mean salary\n",
    "    'term': 'nunique',\n",
    "    'n_obs': 'sum',\n",
    "    'n_skill': 'sum',\n",
    "    'n_control': 'sum',\n",
    "    # 'small_sample': 'mean'  # Percentage of skills with small samples\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten the MultiIndex columns\n",
    "summary_by_category_sig.columns = ['_'.join(col).strip('_') for col in summary_by_category_sig.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_by_category_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for clarity\n",
    "summary_by_category_sig = summary_by_category_sig.rename(columns={\n",
    "    'wage_diff_pct_mean': 'avg_wage_premium_pct',\n",
    "    'wage_diff_pct_median': 'median_wage_premium_pct',\n",
    "    'wage_diff_pct_min': 'min_wage_premium_pct',\n",
    "    'wage_diff_pct_max': 'max_wage_premium_pct',\n",
    "    'dollar_premium_mean': 'avg_dollar_premium',\n",
    "    'dollar_premium_median': 'median_dollar_premium',\n",
    "    'dollar_premium_min': 'min_dollar_premium',\n",
    "    'dollar_premium_max': 'max_dollar_premium',\n",
    "    'term_nunique': 'skill_count',\n",
    "    'n_obs_sum': 'total_observations',\n",
    "    'n_skill_sum': 'treatment_observations',\n",
    "    'n_control_sum': 'control_observations',\n",
    "    # 'small_sample_mean': 'pct_small_samples'\n",
    "})\n",
    "\n",
    "# Add treatment ratio\n",
    "summary_by_category_sig['treatment_ratio'] = summary_by_category_sig['treatment_observations'] / summary_by_category_sig['total_observations']\n",
    "\n",
    "# Add a new rank column within each SOC title, based on avg_wage_premium_pct (descending order)\n",
    "summary_by_category_sig['category_rank_within_soc'] = summary_by_category_sig.groupby(['soc_code', 'soc_title'])['avg_wage_premium_pct'].rank(ascending=False)\n",
    "\n",
    "# Save the category summary with only significant results\n",
    "sig_summary_file = f'ADE_wage_premiums_significant_summary_by_category_SOC5FE_{timestamp}.csv'\n",
    "summary_by_category_sig.to_csv(sig_summary_file, index=False)\n",
    "print(f\"Saved significant results category summary to {sig_summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_by_category_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = summary_by_category_sig\n",
    "\n",
    "# Display \"ALL SOCs\" seperately\n",
    "# Get unique SOC titles including \"All SOCs\" for faceting\n",
    "soc_titles = np.append([\"All SOCs\"], df[df['soc_title'] != 'All SOCs']['soc_title'].unique())\n",
    "\n",
    "# Define a custom color palette for categories\n",
    "categories = df['Category Name'].unique()\n",
    "category_colors = {\n",
    "    'Adaptive & Reflective Rationality': '#3498db',                  # Blue\n",
    "    'Quantitative & Statistical Reasoning': '#2ecc71',               # Green\n",
    "    'Structured & Systematic Decision-Making Process': '#e74c3c'     # Red\n",
    "}\n",
    "\n",
    "# Set up the main plot with more vertical space per facet\n",
    "fig, axes = plt.subplots(len(soc_titles), 1, figsize=(14, 5 * len(soc_titles)), sharex=True)\n",
    "\n",
    "# Create more active title - now positioned closer to the plot\n",
    "active_title = \"Quantitative & Statistical Reasoning Skills Drive Higher Wage Premiums Across Occupations\"\n",
    "fig.suptitle(active_title, fontsize=20, y=1.01, fontweight='bold')\n",
    "\n",
    "# Add a subtitle for context - positioned closer to the plot\n",
    "subtitle = \"Analysis of statistically significant wage premiums by occupation and skill category\"\n",
    "plt.figtext(0.5, 0.975, subtitle, ha='center', fontsize=14, style='italic')\n",
    "\n",
    "# Create the legend at the top - positioned between title and subtitle\n",
    "legend_handles = [plt.Rectangle((0,0), 1, 1, color=category_colors[cat]) for cat in categories]\n",
    "fig.legend(legend_handles, categories, \n",
    "           loc='upper center', \n",
    "           bbox_to_anchor=(0.5, 1.04),  # Position above the plot but below the title\n",
    "           ncol=len(categories), \n",
    "           fontsize=12, \n",
    "           frameon=True)\n",
    "\n",
    "# Loop through each SOC title to create facets\n",
    "for i, soc_title in enumerate(soc_titles):\n",
    "    # Filter data for this SOC\n",
    "    soc_data = df[df['soc_title'] == soc_title].copy()\n",
    "    \n",
    "    # Sort by wage premium (descending) for better visualization\n",
    "    soc_data = soc_data.sort_values('avg_wage_premium_pct', ascending=False)  # Updated column name\n",
    "    \n",
    "    # Get the correct axis (handle both single and multiple subplots)\n",
    "    ax = axes[i] if len(soc_titles) > 1 else axes\n",
    "    \n",
    "    # Set up y positions with more space between bars\n",
    "    y_positions = np.arange(len(soc_data)) * 2  # Double the spacing between bars\n",
    "    \n",
    "    # Plot the bars\n",
    "    for j, (_, row) in enumerate(soc_data.iterrows()):\n",
    "        category = row['Category Name']\n",
    "        wage_premium = row['avg_wage_premium_pct']  # Updated column name\n",
    "        bar_color = category_colors[category]\n",
    "        \n",
    "        # Plot bar with increased height\n",
    "        bar = ax.barh(y_positions[j], wage_premium, height=0.8, color=bar_color, alpha=0.8)\n",
    "        \n",
    "        # Position for annotations depends on whether wage premium is positive or negative\n",
    "        text_x_position = max(wage_premium + 0.01, 0.01) if wage_premium >= 0 else min(wage_premium - 0.01, -0.01)\n",
    "        text_alignment = 'left' if wage_premium >= 0 else 'right'\n",
    "        \n",
    "        # Add category name with better positioning - moved further left\n",
    "        ax.text(-0.40, y_positions[j], f\"{category}\", va='center', ha='left', fontsize=11, \n",
    "                fontweight='bold', color='black')\n",
    "        \n",
    "        # Add wage premium value\n",
    "        ax.text(text_x_position, y_positions[j], f\"{wage_premium:.3f}\", va='center', ha=text_alignment, \n",
    "                fontsize=11, color='black', fontweight='bold')\n",
    "        \n",
    "        # Add rank, skill count and observations as a small annotation with better positioning - moved further left\n",
    "        annotation = f\"Rank: {int(row['category_rank_within_soc'])} | Skills: {int(row['skill_count'])} | Obs: {int(row['total_observations']):,}\"\n",
    "        ax.text(-0.40, y_positions[j]-0.4, annotation, va='center', ha='left', fontsize=9, \n",
    "                color='dimgray')\n",
    "    \n",
    "    # Make the \"All SOCs\" panel stand out with a different background if it's the \"All SOCs\" panel\n",
    "    if soc_title == \"All SOCs\":\n",
    "        ax.set_facecolor('#f8f9fa')  # Light gray background\n",
    "        ax.set_title(f\"Across All Occupations (All SOCs)\", fontsize=16, pad=10, fontweight='bold')\n",
    "    else:\n",
    "        ax.set_title(f\"{soc_title}\", fontsize=14, pad=10)\n",
    "    \n",
    "    # Add zero line\n",
    "    ax.axvline(x=0, color='black', linestyle='-', alpha=0.5, linewidth=0.8)\n",
    "    \n",
    "    # Remove y-ticks since we have labels\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Set y-axis limits to ensure proper spacing\n",
    "    ax.set_ylim(min(y_positions)-1.5, max(y_positions)+1.5)\n",
    "    \n",
    "    # Add grid lines for easier reading\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add x-label only to the bottom subplot\n",
    "    if i == len(soc_titles) - 1:\n",
    "        ax.set_xlabel('Wage Premium (%)', fontsize=12)\n",
    "    \n",
    "    # Extend x-axis to make more room for labels on the left\n",
    "    max_abs_value = df['avg_wage_premium_pct'].abs().max()  # Updated column name\n",
    "    buffer = max_abs_value * 0.3  # Add 30% buffer\n",
    "    ax.set_xlim(-0.45, max_abs_value+buffer)  # Extended left side further for labels\n",
    "\n",
    "# Key adjustments to reduce space between subtitle and charts:\n",
    "plt.tight_layout(rect=[0, 0.02, 1, 0.93])  # Increased top from 0.95 to 0.93 to bring charts up\n",
    "plt.subplots_adjust(hspace=0.4, top=0.93)  # Reduced from 0.6 to 0.4 for less space between subplots\n",
    "\n",
    "# Save figure\n",
    "plt.savefig('wage_premium_by_occupation.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "output_dir = 'occupation_charts'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get unique SOC titles including \"All SOCs\" for separate charts\n",
    "soc_titles = np.append([\"All SOCs\"], df[df['soc_title'] != 'All SOCs']['soc_title'].unique())\n",
    "\n",
    "# Define a custom color palette for categories\n",
    "categories = df['Category Name'].unique()\n",
    "category_colors = {\n",
    "    'Adaptive & Reflective Rationality': '#3498db',                  # Blue\n",
    "    'Quantitative & Statistical Reasoning': '#2ecc71',               # Green\n",
    "    'Structured & Systematic Decision-Making Process': '#e74c3c'     # Red\n",
    "}\n",
    "\n",
    "# Loop through each SOC title to create separate charts\n",
    "for soc_title in soc_titles:\n",
    "    # Filter data for this SOC\n",
    "    soc_data = df[df['soc_title'] == soc_title].copy()\n",
    "    \n",
    "    # Check if we have data for this SOC\n",
    "    if len(soc_data) == 0:\n",
    "        print(f\"No data found for {soc_title}, skipping...\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Creating chart for {soc_title} with {len(soc_data)} data points\")\n",
    "    \n",
    "    # Sort by wage premium (descending) for better visualization\n",
    "    soc_data = soc_data.sort_values('avg_wage_premium_pct', ascending=False)  # Updated column name\n",
    "    \n",
    "    # Create new figure for each SOC - using plt.figure() to ensure a fresh figure\n",
    "    plt.close('all')  # Close any existing figures\n",
    "    fig = plt.figure(figsize=(14, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Set up y positions with more space between bars\n",
    "    y_positions = np.arange(len(soc_data)) * 2  # Double the spacing between bars\n",
    "    \n",
    "    # Print debug info\n",
    "    print(f\"Y positions: {y_positions}\")\n",
    "    \n",
    "    # Plot the bars\n",
    "    for j, (_, row) in enumerate(soc_data.iterrows()):\n",
    "        category = row['Category Name']\n",
    "        # The wage premium is already in percentage format from your updated column\n",
    "        wage_premium = row['avg_wage_premium_pct']  # Updated column name\n",
    "        \n",
    "        print(f\"  - Category: {category}, Premium: {wage_premium}%\")\n",
    "        \n",
    "        # Floor negative values at 0 for display\n",
    "        display_wage_premium = max(0, wage_premium)\n",
    "        \n",
    "        bar_color = category_colors[category]\n",
    "        \n",
    "        # Plot bar with increased height\n",
    "        bar = ax.barh(y_positions[j], display_wage_premium, height=0.8, color=bar_color, alpha=0.8)\n",
    "        \n",
    "        # Position for annotations depends on whether wage premium is positive or negative\n",
    "        text_x_position = max(display_wage_premium + 0.01, 0.01)\n",
    "        \n",
    "        # Add category name with better positioning - moved further left\n",
    "        ax.text(-40, y_positions[j], f\"{category}\", va='center', ha='left', fontsize=11, \n",
    "                fontweight='bold', color='black')\n",
    "        \n",
    "        # Add wage premium value or N/A for negative values\n",
    "        if wage_premium < 0:\n",
    "            ax.text(text_x_position, y_positions[j], \"N/A\", va='center', ha='left', \n",
    "                    fontsize=11, color='black', fontweight='bold')\n",
    "        else:\n",
    "            ax.text(text_x_position, y_positions[j], f\"{wage_premium:.1f}%\", va='center', ha='left', \n",
    "                    fontsize=11, color='black', fontweight='bold')\n",
    "        \n",
    "        # Add rank, skill count and observations as a small annotation with better positioning\n",
    "        annotation = f\"Rank: {int(row['category_rank_within_soc'])} | Skills: {int(row['skill_count'])} | Obs: {int(row['total_observations']):,}\"\n",
    "        ax.text(-40, y_positions[j]-0.4, annotation, va='center', ha='left', fontsize=9, \n",
    "                color='dimgray')\n",
    "    \n",
    "    # Set background style for \"All SOCs\" panel\n",
    "    if soc_title == \"All SOCs\":\n",
    "        ax.set_facecolor('#f8f9fa')  # Light gray background\n",
    "        chart_title = \"Across All Occupations (All SOCs)\"\n",
    "    else:\n",
    "        chart_title = soc_title\n",
    "    \n",
    "    # Add title and subtitle\n",
    "    plt.suptitle(chart_title, fontsize=16, y=0.98, fontweight='bold')\n",
    "    plt.title(\"Wage Premiums by Skill Category\", fontsize=14, pad=10)\n",
    "    \n",
    "    # Add zero line\n",
    "    ax.axvline(x=0, color='black', linestyle='-', alpha=0.5, linewidth=0.8)\n",
    "    \n",
    "    # Remove y-ticks since we have labels\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    # Set y-axis limits to ensure proper spacing\n",
    "    if len(y_positions) > 0:\n",
    "        ax.set_ylim(min(y_positions)-1.5, max(y_positions)+1.5)\n",
    "    \n",
    "    # Add grid lines for easier reading\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add x-label\n",
    "    ax.set_xlabel('Wage Premium (%)', fontsize=12)\n",
    "    \n",
    "    # Create legend at the top\n",
    "    legend_handles = [plt.Rectangle((0,0), 1, 1, color=category_colors[cat]) for cat in categories]\n",
    "    ax.legend(legend_handles, categories, \n",
    "              loc='upper right',\n",
    "              fontsize=10, \n",
    "              frameon=True)\n",
    "    \n",
    "    # Extend x-axis to make more room for labels on the left\n",
    "    max_premium = max(df['avg_wage_premium_pct'].max(), 5)  # Updated column name, no need to multiply by 100\n",
    "    buffer = max_premium * 0.3  # Add 30% buffer\n",
    "    ax.set_xlim(-45, max_premium+buffer)  # Extended left side further for labels\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    # Create filename-safe version of SOC title\n",
    "    safe_filename = soc_title.replace('/', '_').replace(' ', '_').replace(',', '').replace('&', 'and')\n",
    "    filename = f'{output_dir}/{safe_filename}_wage_premium.png'\n",
    "    \n",
    "    # Ensure the figure is drawn before saving\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # Save figure with debugging\n",
    "    print(f\"Saving figure to {filename}\")\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    # Show figure\n",
    "    plt.show()\n",
    "    \n",
    "    # Close the figure explicitly\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f\"All charts saved to {output_dir}/ directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for only statistically significant results (p-value < 0.05)\n",
    "sig_df = detailed_df[detailed_df['p.value'] < 0.05].copy()\n",
    "\n",
    "# Step 1: Pivot the data to get skills as rows and SOC codes as columns\n",
    "# First, exclude the \"All SOCs\" category to focus on specific occupations\n",
    "soc_specific_df = sig_df[sig_df['soc_code'] != 'All']\n",
    "\n",
    "# Create the pivot table with wage_diff_pct as values\n",
    "pivot_df = soc_specific_df.pivot_table(\n",
    "    index='term',\n",
    "    columns='soc_code',\n",
    "    values='wage_diff_pct',\n",
    "    aggfunc='first'  # Use first since we should have one value per skill-SOC combination\n",
    ")\n",
    "\n",
    "# Step 2: Calculate the range of wage premiums for each skill across SOCs\n",
    "# This measures how much the premium varies between occupations\n",
    "pivot_df['premium_range'] = pivot_df.max(axis=1) - pivot_df.min(axis=1)\n",
    "\n",
    "# Step 3: Identify skills that have both positive and negative premiums\n",
    "# Create indicators for positive and negative premiums\n",
    "for col in pivot_df.columns:\n",
    "    if col != 'premium_range':\n",
    "        pivot_df[f'{col}_is_positive'] = (pivot_df[col] > 0)\n",
    "        pivot_df[f'{col}_is_negative'] = (pivot_df[col] < 0)\n",
    "\n",
    "# Count how many positive and negative premiums each skill has\n",
    "positive_cols = [col for col in pivot_df.columns if col.endswith('_is_positive')]\n",
    "negative_cols = [col for col in pivot_df.columns if col.endswith('_is_negative')]\n",
    "\n",
    "pivot_df['positive_count'] = pivot_df[positive_cols].sum(axis=1)\n",
    "pivot_df['negative_count'] = pivot_df[negative_cols].sum(axis=1)\n",
    "\n",
    "# Filter for skills that have both positive and negative premiums\n",
    "contrasting_skills = pivot_df[(pivot_df['positive_count'] > 0) & (pivot_df['negative_count'] > 0)]\n",
    "\n",
    "# Step 4: Sort by the range to find skills with the largest differences\n",
    "contrasting_skills = contrasting_skills.sort_values('premium_range', ascending=False)\n",
    "\n",
    "# Drop the indicator columns for cleaner output\n",
    "cols_to_drop = positive_cols + negative_cols + ['positive_count', 'negative_count']\n",
    "contrasting_skills = contrasting_skills.drop(columns=cols_to_drop)\n",
    "\n",
    "# Step 5: Get the top skills with contrasting effects\n",
    "top_contrasting_skills = contrasting_skills.head(20)\n",
    "\n",
    "# To see which SOCs have positive vs negative for each skill, create a summary\n",
    "skill_contrast_summary = []\n",
    "\n",
    "for skill in top_contrasting_skills.index:\n",
    "    skill_data = pivot_df.loc[skill].drop(['premium_range', 'positive_count', 'negative_count'] + \n",
    "                                          positive_cols + negative_cols)\n",
    "    \n",
    "    # Find the SOC code with the highest positive premium\n",
    "    max_positive_soc = skill_data[skill_data > 0].idxmax() if any(skill_data > 0) else None\n",
    "    min_negative_soc = skill_data[skill_data < 0].idxmin() if any(skill_data < 0) else None\n",
    "    \n",
    "    # Get the SOC title for the highest positive premium\n",
    "    if max_positive_soc:\n",
    "        max_positive_title = soc_specific_df[soc_specific_df['soc_code'] == max_positive_soc]['soc_title'].iloc[0]\n",
    "    else:\n",
    "        max_positive_title = \"\"\n",
    "    \n",
    "    # Get the SOC title for the most negative premium\n",
    "    if min_negative_soc:\n",
    "        min_negative_title = soc_specific_df[soc_specific_df['soc_code'] == min_negative_soc]['soc_title'].iloc[0]\n",
    "    else:\n",
    "        min_negative_title = \"\"\n",
    "    \n",
    "    # Get the highest positive and most negative values\n",
    "    max_positive = skill_data[skill_data > 0].max() if any(skill_data > 0) else 0\n",
    "    min_negative = skill_data[skill_data < 0].min() if any(skill_data < 0) else 0\n",
    "    \n",
    "    skill_contrast_summary.append({\n",
    "        'Skill': skill,\n",
    "        'Premium Range': contrasting_skills.loc[skill, 'premium_range'],\n",
    "        'Highest Positive': max_positive,\n",
    "        'Most Negative': min_negative,\n",
    "        'Positive SOCs': max_positive_title,\n",
    "        'Negative SOCs': min_negative_title\n",
    "    })\n",
    "\n",
    "# Create a summary dataframe\n",
    "contrast_summary_df = pd.DataFrame(skill_contrast_summary)\n",
    "\n",
    "# Save the results\n",
    "contrast_summary_df.to_csv('skills_with_contrasting_premiums.csv', index=False)\n",
    "top_contrasting_skills.to_csv('top_contrasting_skills_detailed.csv')\n",
    "\n",
    "print(f\"Found {len(contrasting_skills)} skills with both positive and negative wage premiums across occupations.\")\n",
    "print(f\"Top skill with contrasting premiums: {contrast_summary_df.iloc[0]['Skill']} with a premium range of {contrast_summary_df.iloc[0]['Premium Range']:.4f}\")\n",
    "\n",
    "# Display the summary table\n",
    "contrast_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chart_from_summary(summary_df, n_skills=5):\n",
    "    \"\"\"\n",
    "    Create a diverging bar chart from the summary dataframe with fixed SOC title handling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    summary_df : pandas DataFrame\n",
    "        Your contrast_summary_df with Skill, Highest Positive, Most Negative, etc.\n",
    "    n_skills : int\n",
    "        Number of top skills to display\n",
    "    \"\"\"\n",
    "    import textwrap\n",
    "    \n",
    "    # Get top N skills\n",
    "    top_skills = summary_df.head(n_skills)\n",
    "    \n",
    "    # Create a figure with a subplot for each skill\n",
    "    fig, axes = plt.subplots(n_skills, 1, figsize=(18, 3*n_skills), sharex=True)\n",
    "    \n",
    "    # Ensure axes is always a list even with a single skill\n",
    "    if n_skills == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # For each skill, create a simplified horizontal bar chart showing highest positive and most negative\n",
    "    for i, (_, row) in enumerate(top_skills.iterrows()):\n",
    "        ax = axes[i]\n",
    "        skill = row['Skill']\n",
    "        \n",
    "        # Get highest positive occupation\n",
    "        pos_val = row['Highest Positive']\n",
    "        pos_soc = row['Positive SOCs']  # Keep the entire string\n",
    "        \n",
    "        # Get most negative occupation\n",
    "        neg_val = row['Most Negative']\n",
    "        neg_soc = row['Negative SOCs']  # Keep the entire string\n",
    "        \n",
    "        # Create a dictionary of values\n",
    "        values = {\n",
    "            'Highest Positive\\n' + pos_soc: pos_val,\n",
    "            'Most Negative\\n' + neg_soc: neg_val\n",
    "        }\n",
    "        \n",
    "        # Sort by value\n",
    "        sorted_items = sorted(values.items(), key=lambda x: x[1])\n",
    "        labels = [item[0] for item in sorted_items]\n",
    "        vals = [item[1] for item in sorted_items]\n",
    "        \n",
    "        # Set up positions for the bars\n",
    "        y_pos = np.arange(len(labels))\n",
    "        \n",
    "        # Create bars with different colors\n",
    "        bars = ax.barh(y_pos, vals, height=0.6)\n",
    "        bars[0].set_color('#d62728')  # Red for negative\n",
    "        bars[1].set_color('#2ca02c')  # Green for positive\n",
    "        \n",
    "        # Remove the y-ticks completely\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # Custom positioning for the labels - moved further left\n",
    "        for j, label in enumerate(labels):\n",
    "            # Split label into two parts: title and SOC\n",
    "            parts = label.split('\\n')\n",
    "            if len(parts) == 2:\n",
    "                title, soc = parts\n",
    "                # First line (title) in bold - moved much further left\n",
    "                ax.text(-1.5, y_pos[j], title, ha='left', va='bottom', fontsize=10, fontweight='bold')\n",
    "                # Second line (SOC) with smaller font, wrapped to multiple lines if needed\n",
    "                wrapped_soc = '\\n'.join(textwrap.wrap(soc, width=40))  # Wrap text at 40 characters\n",
    "                ax.text(-1.5, y_pos[j]-0.15, wrapped_soc, ha='left', va='top', fontsize=9)\n",
    "            else:\n",
    "                # Fallback if there's no newline\n",
    "                ax.text(-1.5, y_pos[j], label, ha='left', va='center', fontsize=10)\n",
    "        \n",
    "        # Add values inside/near the bars with better positioning\n",
    "        for j, v in enumerate(vals):\n",
    "            if v < 0:\n",
    "                # For negative values, position inside the bar near the left edge\n",
    "                x_pos = v * 0.5  # Position at middle of bar\n",
    "                ha = 'center'\n",
    "                color = 'white'  # White text for contrast against red bar\n",
    "            else:\n",
    "                # For positive values, position to the right of the bar\n",
    "                x_pos = v + 0.02\n",
    "                ha = 'left'\n",
    "                color = 'black'\n",
    "            \n",
    "            ax.text(x_pos, y_pos[j], f'{v:.3f}', va='center', ha=ha, fontsize=10, \n",
    "                    fontweight='bold', color=color)\n",
    "        \n",
    "        # Set title and labels for this subplot\n",
    "        ax.set_title(f'{skill}', fontsize=14, pad=10)\n",
    "        \n",
    "        # Add a premium range annotation\n",
    "        range_text = f\"Premium Range: {row['Premium Range']:.3f}\"\n",
    "        ax.annotate(range_text, xy=(0.98, 0.05), xycoords='axes fraction', \n",
    "                    ha='right', fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", \n",
    "                                                      fc='#f0f0f0', ec=\"gray\", alpha=0.7))\n",
    "        \n",
    "        # Add gridlines\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "        \n",
    "        # Add a vertical line at x=0\n",
    "        ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
    "        \n",
    "        # Set x axis limits to ensure bars are clearly visible and leave room for labels on the left\n",
    "        max_abs = max(abs(min(vals)), abs(max(vals)))\n",
    "        buffer = max_abs * 0.3\n",
    "        ax.set_xlim(-1.6, max_abs+buffer)  # Extended leftward to make room for labels\n",
    "    \n",
    "    # Add a common x-label for all subplots\n",
    "    fig.text(0.5, 0.02, 'Wage Premium', ha='center', fontsize=14)\n",
    "    \n",
    "    # Add a title for the entire figure\n",
    "    fig.suptitle('Skills with Most Contrasting Wage Premiums Across Occupations', \n",
    "                 fontsize=16, y=1.02, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig('contrasting_skills_summary_bar.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_chart_from_summary(contrast_summary_df, n_skills=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "output_dir = 'occupation_charts'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set style for better-looking charts\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# For Jupyter notebooks, use this magic command\n",
    "%matplotlib inline\n",
    "\n",
    "# First, create a filtered dataset of significant results\n",
    "significant_reliable = df_with_categories[\n",
    "    (detailed_df['p.value'] < 0.05) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart 1: Weighted average wage premium by category\n",
    "# First, filter for significant results without small samples\n",
    "# AND exclude \"All SOCs\" to avoid double counting\n",
    "significant_reliable_by_soc = significant_reliable[\n",
    "    significant_reliable['soc_code'] != 'All'  # Exclude the \"All SOCs\" results\n",
    "]\n",
    "\n",
    "# Calculate the weighted average by category\n",
    "# Using n_skill as weights\n",
    "category_weighted = significant_reliable_by_soc.groupby('Category Name').apply(\n",
    "    lambda x: np.average(x['wage_diff_pct'], weights=x['n_skill'])\n",
    ").reset_index()\n",
    "category_weighted.columns = ['Category Name', 'weighted_wage_premium']\n",
    "\n",
    "# Create a percentage version of the weighted_wage_premium for plotting\n",
    "category_weighted['weighted_wage_premium_pct'] = category_weighted['weighted_wage_premium'] * 100\n",
    "\n",
    "# Sort by premium descending\n",
    "category_weighted = category_weighted.sort_values('weighted_wage_premium', ascending=False)\n",
    "\n",
    "# Create the chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "barplot = sns.barplot(\n",
    "    x='weighted_wage_premium_pct',  # Use the percentage column for the x-axis\n",
    "    y='Category Name', \n",
    "    data=category_weighted,\n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "# Add value labels with percentages\n",
    "for i, v in enumerate(category_weighted['weighted_wage_premium_pct']):\n",
    "    barplot.text(v + 0.2, i, f\"{v:.1f}%\", va='center')  # Adjusted position and format\n",
    "\n",
    "plt.title('Weighted Average Wage Premium by Skill Category', fontsize=16)\n",
    "plt.xlabel('Wage Premium (%)', fontsize=14)\n",
    "plt.ylabel('Skill Category', fontsize=14)\n",
    "\n",
    "# Add a note about weighting methodology\n",
    "plt.figtext(0.5, 0.02, \"Note: Wage premiums are weighted by the number of skills in each category\", \n",
    "            ha='center', fontsize=12, fontstyle='italic')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])  # Adjust layout to make room for the note\n",
    "\n",
    "# Save the chart first, then display it\n",
    "plt.savefig('occupation_charts/wage_premium_by_category_weighted.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close('all')  # Close all figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output directory exists\n",
    "output_dir = 'occupation_charts'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set style for better-looking charts\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# For Jupyter notebooks, use this magic command\n",
    "%matplotlib inline\n",
    "\n",
    "# First, create a filtered dataset of significant results \n",
    "significant_reliable = detailed_df[\n",
    "    (detailed_df['p.value'] < 0.05) \n",
    "]\n",
    "\n",
    "# Filter for significant results in \"All SOCs\" only\n",
    "all_socs_sig = significant_reliable[significant_reliable['soc_code'] == 'All']\n",
    "\n",
    "# Sort by wage premium\n",
    "all_socs_sig_sorted = all_socs_sig.sort_values('wage_diff_pct', ascending=False)\n",
    "\n",
    "# Get top 5, middle 5, and bottom 5\n",
    "top_5 = all_socs_sig_sorted.head(5)\n",
    "middle_5 = all_socs_sig_sorted.iloc[len(all_socs_sig_sorted)//2-2:len(all_socs_sig_sorted)//2+3]\n",
    "bottom_5 = all_socs_sig_sorted.tail(5).copy()  # Create a copy to avoid warning\n",
    "\n",
    "# Modify: Floor negative wage premiums at 0 for bottom skills while preserving the original order\n",
    "# Create a display column for plotting that floors negative values at 0\n",
    "bottom_5['wage_diff_pct_display'] = bottom_5['wage_diff_pct'].clip(lower=0)\n",
    "\n",
    "# Combine with a group label\n",
    "top_5['group'] = 'Top 5'\n",
    "middle_5['group'] = 'Middle 5'\n",
    "bottom_5['group'] = 'Bottom 5'\n",
    "\n",
    "# Create display column for top and middle as well (same as original value)\n",
    "top_5['wage_diff_pct_display'] = top_5['wage_diff_pct']\n",
    "middle_5['wage_diff_pct_display'] = middle_5['wage_diff_pct']\n",
    "\n",
    "# Combine the dataframes\n",
    "combined = pd.concat([top_5, middle_5, bottom_5])\n",
    "\n",
    "# Create the chart using the display column for x-axis\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Convert wage premium to percentage for display (multiply by 100)\n",
    "for df in [top_5, middle_5, bottom_5]:\n",
    "    df['wage_diff_pct_display_pct'] = df['wage_diff_pct_display'] * 100\n",
    "\n",
    "# Combine the dataframes\n",
    "combined = pd.concat([top_5, middle_5, bottom_5])\n",
    "\n",
    "# Plot all bars first (this will ensure all y-labels are shown)\n",
    "barplot = sns.barplot(\n",
    "    x='wage_diff_pct_display_pct',  # Use percentage values\n",
    "    y='term', \n",
    "    data=combined,\n",
    "    hue='group',\n",
    "    palette={\n",
    "        'Top 5': 'forestgreen',\n",
    "        'Middle 5': 'gold',\n",
    "        'Bottom 5': 'lightgray'  # Light gray for bottom 5\n",
    "    },\n",
    "    dodge=False\n",
    ")\n",
    "\n",
    "# Add hatching to the bottom 5 bars\n",
    "bottom_hatch = '///'\n",
    "for i, row in enumerate(combined.itertuples()):\n",
    "    if row.group == 'Bottom 5':\n",
    "        # Get the bar and add hatching\n",
    "        bar = barplot.patches[i]\n",
    "        bar.set_hatch(bottom_hatch)\n",
    "        bar.set_edgecolor('firebrick')\n",
    "        bar.set_linewidth(1.5)\n",
    "\n",
    "# Add value labels (single set of labels to avoid duplicates)\n",
    "for i, row in enumerate(combined.itertuples()):\n",
    "    if row.group == 'Bottom 5':\n",
    "        # Show \"N/A\" for bottom 5\n",
    "        barplot.text(0.1, i, \"N/A\", va='center', fontweight='bold')\n",
    "    else:\n",
    "        # Show the original value formatted as percentage\n",
    "        # Convert decimal to percentage for the label\n",
    "        pct_value = row.wage_diff_pct * 100\n",
    "        barplot.text(pct_value + 0.2, i, f\"{pct_value:.1f}%\", va='center')\n",
    "\n",
    "plt.title('Top, Middle, and Bottom 5 Skills by Wage Premium', fontsize=16)\n",
    "plt.xlabel('Wage Premium (%)', fontsize=14)\n",
    "plt.ylabel('Skill', fontsize=14)\n",
    "plt.axvline(x=0, color='black', linestyle='-', alpha=0.7)\n",
    "plt.legend(title='Group')\n",
    "\n",
    "# Add a note about N/A skills\n",
    "plt.figtext(0.5, 0.02, \"Note: N/A skills represent those with no wage premium\", \n",
    "            ha='center', fontsize=12, fontstyle='italic')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])  # Adjust layout to make room for the note\n",
    "\n",
    "# Save the chart first, then display it\n",
    "plt.savefig('occupation_charts/top_middle_bottom_skills_all.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close('all')  # Close all figures\n",
    "\n",
    "print(\"Chart created successfully and saved as:\")\n",
    "print(\"- occupation_charts/top_middle_bottom_skills_all.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 skills with highest wage premium across all SOCs\n",
    "top_skills_overall = detailed_df[detailed_df['soc_code'] == 'All'].nlargest(10, 'wage_diff_pct')\n",
    "print(top_skills_overall[['term', 'wage_diff_pct']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test the sampling error (results 1 and 2 should be the same if we uncomment out the 500,000 sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing batch 1, skills 1 to 1\n",
      "  Processing skill: Adaptive Reasoning\n"
     ]
    }
   ],
   "source": [
    "# Run 1\n",
    "results1 = analyze_by_soc(reg_df, [\"Adaptive Reasoning\"], batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df_new = pd.read_sql(combined_query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 2 immediately after  \n",
    "results2 = analyze_by_soc(reg_df_new, [\"Adaptive Reasoning\"], batch_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
